{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e689afa4",
   "metadata": {},
   "source": [
    "# What is linear algebra?\n",
    "\n",
    "**Linear algebra** is the study of systems of *linear* equations. \n",
    "\n",
    "After completing this notebook, you'll be able to:\n",
    "* Create a vector using NumPy, compute its length, and visualize it\n",
    "* Compute the dot product between two vectors\n",
    "* Normalize the dot product to calculate a correlation coeffeicient\n",
    "* Construct and multiply matrices in Python\n",
    "* Create and manipulate special cases of matrices (unit matrix, diagonal matrix)\n",
    "* Explain matrices as a linear transformation and relate matrix properties to properties of that linear transformation\n",
    "* Define what eigenvalues/eigenvectors are and determine them using Python\n",
    "\n",
    "<hr>\n",
    "\n",
    "## Setup\n",
    "Below, we'll import a custom module with some **helper functions** to easily visualize vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54e6752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules, including a custom one!\n",
    "from Modules import linear_algebra, matrices\n",
    "import numpy as np\n",
    "\n",
    "# Check imports\n",
    "%whos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0814608e",
   "metadata": {},
   "source": [
    "## Vectors\n",
    "\n",
    "We can build **vectors**, a one-dimensional array of numbers, using NumPy. Below, we'll generate a row vector and column vector. We will also check the vector's *mathematical* **dimensionality** (using `len()` or the `shape` attribute), compared to its dimensionality as a NumPy array (using the `ndim` attribute).\n",
    "\n",
    "**Note**: A vector created without the extra brackets (e.g. `np.array([1,2,3])`) would be orientationless. Sometimes this is okay, but sometimes we need to be extra clear about the orientation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bca248",
   "metadata": {},
   "outputs": [],
   "source": [
    "array = np.array([1,2,3])\n",
    "\n",
    "# Check ndim and shape\n",
    "print(array.ndim)\n",
    "print(array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b0db55",
   "metadata": {},
   "outputs": [],
   "source": [
    "row_vector = np.array([[ -1, 0, 2, 3.1] ])\n",
    "\n",
    "# Check ndim and shape\n",
    "\n",
    "\n",
    "row_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f7108d",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_vector =  np.array([ [1],[2],[3] ])\n",
    "column_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba14847",
   "metadata": {},
   "source": [
    "### Transposing vectors (or matrices)\n",
    "\n",
    "To transpose a vector or matrix, we can use [`transpose()`](https://numpy.org/doc/stable/reference/generated/numpy.matrix.transpose.html) or simply`T`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b9c2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transpose our vectors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62db4fe",
   "metadata": {},
   "source": [
    "### Coordinate vectors\n",
    "When our origin is at zero, we can also use arrays to store coordinates. Below, we'll use NumPy to create two arrays which represent the coordinates for two vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2a362c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that here we don't need to worry about vector orientation (row vs. column), \n",
    "# so for simplicity the vectors are created orientationless.\n",
    "\n",
    "v1 = np.array([2, 4])\n",
    "v2 = np.array([2, -2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec401c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the function we imported to visualize\n",
    "linear_algebra.visualize_vectors(v1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580e613e",
   "metadata": {},
   "source": [
    "### Vector addition, subtraction, and scalar multiplication\n",
    "\n",
    "<div class=\"alert alert-success\"><b>Tasks:</b> \n",
    "    \n",
    "* Add vectors v1 and v2. \n",
    "* Subtract vectors v1 and v2. \n",
    "* Multiply v2 by a scalar of 3.\n",
    "    \n",
    "    For each of these, you can use our <code>visualize_vectors</code> function to see the resulting array.\n",
    "    \n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624779ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manipulate vectors here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c719c9",
   "metadata": {},
   "source": [
    "### Normalizing vectors\n",
    "\n",
    "A vector is **normalized** when *each element* in the vector is divided by the length. Note that length here *is not* how many elements are in the vector! It is the *actual* length of the vector. Thankfully, there's a NumPy function that can help us compute length: [`np.linalg.norm()`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.norm.html). The **norm** is a measure of the size or length of a vector or matrix in linear algebra. It is a generalization of the concept of the magnitude of a vector in Euclidean space.\n",
    "\n",
    "Let's see if we can normalize by division."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465376ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "v1_length = np.linalg.norm(v1) # compute norm (aka magnitude or length)\n",
    "v1_length "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09b60dc",
   "metadata": {},
   "source": [
    "We can compare the length we get above to the output of the Pythagoream theorem. *Hint*: use `np.sqrt()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47148a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare norm to Pythagoream\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28509bec",
   "metadata": {},
   "source": [
    "Finally, we will divide the original vector by the length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78aea282",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_vector = v1 / v1_length\n",
    "normalized_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a30c980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the magnitude of the new vector! Is this a unit vector?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fb3fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize v1 alongside the new, unit vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca93bce1",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\"><b>Task:</b> Write a function, <code>normalize_vector</code> that takes any vector and normalizes it. What happens if you give it the zeros vector?</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7f687c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your function here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86cea6e",
   "metadata": {},
   "source": [
    "Finally, we can combine addition, subtraction, and scalars to create linear combinations of vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8afca28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear combinations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d73c11",
   "metadata": {},
   "source": [
    "## Linear Algebra Functions\n",
    "\n",
    "One of the most important operations in linear algebra is the dot product. First, let's show how dot products work between vectors. Below, we'll use `np.dot()`. **Note**: This function technically implements matrix multiplication, a collection of dot products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b3560e",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = np.array([1,2,3,4])\n",
    "w = np.array([5,6,7,8])\n",
    "np.dot(v,w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7560588",
   "metadata": {},
   "source": [
    "### Special dot product observations\n",
    "\n",
    "<div class=\"alert alert-success\"><b>Task:</b> Below, create two vectors that are <b>orthogonal</b>. Then, compute their dot product.</div>\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/8/84/Perpendicular-coloured.svg/220px-Perpendicular-coloured.svg.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16819e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vectors here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97eda12",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\"><b>Task:</b> Is the dot product <b>commutative</b>? Commutative means that $a*b == b*a$. Test this below.</div>\n",
    "\n",
    "(See a full elaboration on this [here](https://youtu.be/LyGKycYT2v0?si=FBmISeSy6Wjs_v22)!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f5ae4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test commutative\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d280bd",
   "metadata": {},
   "source": [
    "### Computing correlations\n",
    "The magnitude of the dot product indicates the strength of similarity between two vectors, but it is *also* is related to the magnitude of the numerical values in the data.\n",
    "\n",
    "Let's demonstrate that using two vectors. One gives the height and weight of two different people in inches and grams, and the second gives the height and weight of those same in feet and pounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6dea004",
   "metadata": {},
   "outputs": [],
   "source": [
    "pounds = np.array([130,150,200,210])\n",
    "grams = pounds*453.592\n",
    "\n",
    "inches = np.array([64,76,90,80])\n",
    "feet = inches/12\n",
    "\n",
    "print(np.dot(inches,grams))\n",
    "print(np.dot(feet,pounds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d105d7",
   "metadata": {},
   "source": [
    "So, if we want to compute a meaningful value that isn't simply reflecting the magnitude of the vectors, we need to normalize these dot products by doing the following:\n",
    "\n",
    "1. Mean centering each variable: subtracting the average value from each data value. *This is the same as computing the norm!*\n",
    "2. Dividing the dot product by the product of the vector norms. This divisive normalization cancels the measurement units and scales maximum possible correlation magnitude to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fd3c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_dot(x,y):\n",
    "\n",
    "    # Mean center each value\n",
    "    x_m  = x-np.mean(x)\n",
    "    y_m  = y-np.mean(y)\n",
    "\n",
    "    num = np.dot(x_m,y_m) # numerator\n",
    "    den = np.linalg.norm(x_m) * np.linalg.norm(y_m) # denominator\n",
    "    cor = num / den\n",
    "    \n",
    "    return cor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1dcac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize_dot(grams,inches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21585eb0",
   "metadata": {},
   "source": [
    "Of course, given that computing a correlation is a very common data analysis approach, this is built into the **stats** module of the SciPy package. We can compare our results above to the use of [`stats.pearsonr()`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.pearsonr.html): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e422c14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "statistic, pvalue = stats.pearsonr(inches,grams)\n",
    "print(statistic, pvalue)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e639f00b",
   "metadata": {},
   "source": [
    "## Matrices\n",
    "\n",
    "A **matrix** is a rectangular array of numbers. The numbers in the matrix are **entries**. We can refer to each of the entries by their row and column.\n",
    "```\n",
    "A = [-1 -6  2\n",
    "      0  8  1]\n",
    "```\n",
    "You will sometimes see matrices referred to in mathematical notation. For example, a matrix $A$ could be denoted by $A = (a_{ij})$ where $a_{ij}$  is the entry in the $i^{th}$ row and $j^{th}$ column of matrix $A$. So for the matrix above, $A_{23}$ == 1.\n",
    "\n",
    "The **size** of the matrix is the number of rows multipled by the number of columns. $A$ is a 2x3 matrix, with a total size of 6. If a matrix has the same number of rows and columns, it is a **square** matrix. If it only has a dimension of one in one direction (e.g. 3x1), it is a **column matrix** (or **column vector**). \n",
    "\n",
    "The **transpose** of a matrix switches its rows and columns. The transpose (^T, or $^T$) of matrix $A$ above would be:\n",
    "```\n",
    "A^T = [-1 0\n",
    "       -6 8\n",
    "        2 1]\n",
    "```\n",
    "\n",
    "### Buiding matrices in NumPy\n",
    "We can build matrices in Python using numpy, using the following notation (notice there are parentheses to denote the array function, with brackets inside to indicate a list, with brackets inside *those* brackets for each row):\n",
    "\n",
    "```\n",
    "my_matrix = np.array([[row_1],[row_2],...[row_n]])\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3b1ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a 3x3 matrix\n",
    "my_matrix = np.array([[1,2,3],[4,5,6],[7,8,9]])\n",
    "my_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9827147e",
   "metadata": {},
   "source": [
    "Other useful matrix functions:\n",
    "* `np.random.randint()` builds a random matrix\n",
    "* `np.eye()` builds an **identity matrix**\n",
    "* `np.zeros()` builds a matrix of zeros\n",
    "* `np.ones()` builds a matrix of ones\n",
    "* `np.diag()` builds a **diagonal matrix**\n",
    "\n",
    "**Note**: Each of these differ in the inputs they take to instruct their size and shape -- always refer to the documentation!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9007b5ad",
   "metadata": {},
   "source": [
    "We can also do matrix multiplication. In Python, we can use the `@` operator for matrix/vector multiplications. We can also use the NumPy [`np.dot`](https://numpy.org/doc/stable/reference/generated/numpy.dot.html#numpy.dot), [`np.matmul`](https://numpy.org/doc/stable/reference/generated/numpy.matmul.html). See the documentation for their differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611337e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a random matrix\n",
    "random_matrix = ...\n",
    "\n",
    "# Use matmul for matrix multiplication\n",
    "np.matmul(my_matrix,random_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b24c0da",
   "metadata": {},
   "source": [
    "### Solving linear equations (abstract example):\n",
    "\n",
    "Let's say we have the following system of linear equations that we'd like to solve:\n",
    "\n",
    "$4x + 3y + 2z = 25$\n",
    "\n",
    "$-2x + 2y + 3z = -10$\n",
    "\n",
    "$3x - 5y + 2z = -4$\n",
    "\n",
    "We can use the inverse method, implemented using `np.linalg.inv()`. The **inverse** of a matrix (e.g., of $M$) is one that can be multiplied by it to produce an identity matrix. This is how we \"cancel\" a matrix in order to solve problems in the form $ Ax = b $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28eed4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "M = np.array([[4,3,2],[-2,2,3],[3,-5,2]]) # 3x3 co-efficients matrix\n",
    "K = np.array([[25],[-10],[-4]])           # column matrix\n",
    "V = np.array([['x'],['y'],['z']])         # unknown matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58fd03e",
   "metadata": {},
   "source": [
    "$ M \\cdot V = K $\n",
    "\n",
    "so,\n",
    "\n",
    "$ V = M^{-1} \\cdot K$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a5192a",
   "metadata": {},
   "outputs": [],
   "source": [
    "V = np.linalg.inv(M).dot(K)\n",
    "V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ee41b6",
   "metadata": {},
   "source": [
    "### Coding Exercise 1.1: Understanding neural transformations using linear equations\n",
    "\n",
    "We will look at a group of 2 LGN neurons which get input from 2 retinal neurons: we will call the population of LGN neurons population $p$. Below, we have the system of linear equations that dictates the neuron models for each population. $r_1$ and $r_2$ correspond to the retinal neural activities (of neuron 1 and 2). $g_{p_1}$ and  $g_{p_2}$ correspond to the responses of the LGN neurons 1 and 2 in population $p$.\n",
    "\n",
    "\\begin{align}\n",
    "r_1 + 3r_2 &= g_{p_1} \\\\\n",
    "2r_1 + r_2 &= g_{p_2}\n",
    "\\end{align}\n",
    "\n",
    "<br>\n",
    "\n",
    "<div class=\"alert alert-success\"><b>Tasks:</b>\n",
    "    \n",
    "1. Cast each equation (i.e., $g_{p_1}$ and $g_{p_2}$) as a matrix-vector multiplication:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{g}_p = \\mathbf{P}\\mathbf{r}\n",
    "\\end{equation}\n",
    "\n",
    "where $P$ is the weight matrix to population $p$.\n",
    "\n",
    "2. Let's say we only recorded from the LGN cells (and know the weight matrix) and are trying to figure out how the retinal cells responded. Solve the matrix equation for the given LGN activities:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{g}_p =\n",
    "\\begin{bmatrix}\n",
    "16 \\\\\n",
    "7\n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "</div>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b972665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create P (using np array)\n",
    "P = ...\n",
    "\n",
    "# Create g_p (using np array)\n",
    "g_p = ...\n",
    "\n",
    "# Solve for r (using np.linalg.inv)\n",
    "r = ...\n",
    "\n",
    "# Print r\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105cc6bf",
   "metadata": {},
   "source": [
    "You can recover how the retinal neurons respond given the weight matrix and LGN responses! You have solved the system of equations using matrices. We can't always do this though: let's say we have a different group of 2 LGN neurons -  population q - with the following weight matrix from the retinal neurons.\n",
    "\n",
    "\\begin{equation}Q =\n",
    "\\begin{bmatrix}\n",
    "4 & 1 \\\\\n",
    "8 & 2\n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "As you can see if you run the next code cell, we get an error if we try to invert this matrix to solve the equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf43f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_q = np.array([16, 7])\n",
    "Q = np.array([[4, 1], [8, 2]])\n",
    "\n",
    "print(np.linalg.inv(Q) @ g_q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40da3259",
   "metadata": {},
   "source": [
    "## Matrices as linear transformations\n",
    "\n",
    "For now, let's start to think about all of this as **[linear transformations of matrices](https://www.youtube.com/watch?v=N6UUV9tVIr8)**. \n",
    "\n",
    "Matrices can be thought of as enacting linear transformations. When multiplied with a vector, they transform it into another vector. In fact, they are transforming a grid of space in a linear manner: the origin stays in place and grid lines remain straight, parallel, and evenly spaced.\n",
    "\n",
    "### Coding Exercise 1.2: Creating matrices for transformations\n",
    "\n",
    "<div class=\"alert alert-success\"><b>Tasks:</b>\n",
    "\n",
    "1. Come up with a matrix $A$ for which the corresponding linear transformation is reflection through the $y$ axis (flipping across the $y$ axis). For example, $\\mathbf{x} = \\begin{bmatrix}\n",
    "2 \\\\\n",
    "6  \\\\\n",
    "\\end{bmatrix}$ should become $\\mathbf{b} = \\begin{bmatrix}\n",
    "-2 \\\\\n",
    "6  \\\\\n",
    "\\end{bmatrix}$ when multiplied with $A$.\n",
    "2. Come up with a matrix $B$ for which the corresponding linear transformation is projecting onto the $x$ axis. For example, $\\bar{x} = \\begin{bmatrix}\n",
    "2 \\\\\n",
    "3  \\\\\n",
    "\\end{bmatrix}$ should become $\\bar{b} = \\begin{bmatrix}\n",
    "2 \\\\\n",
    "0  \\\\\n",
    "\\end{bmatrix}$ when multiplied with $B$.\n",
    "\n",
    "</div>\n",
    "\n",
    "**Remember to think about where your basis vectors should end up! Then your matrix consists of the transformed basis vectors. Drawing out what you want to happen can help**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "023f253e",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = ...\n",
    "\n",
    "# Uncomment to visualize transformation\n",
    "# plot_linear_transformation(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34f5cc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "B = ...\n",
    "\n",
    "# Uncomment to visualize transformation\n",
    "# plot_linear_transformation(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7675a5c5",
   "metadata": {},
   "source": [
    "## Eigenvalues & Eigenvectors\n",
    "\n",
    "[**This video**](https://www.youtube.com/watch?v=l-c7ptT7znM) covers eigenvalues and eigenvectors.\n",
    "\n",
    "Eigenvectors $\\mathbf{v}$ of a matrix $\\mathbf{W}$ are vectors that, when multipled by the matrix, equal a scalar multiple of themselves. That scalar multiple is the corresponding eigenvalue $\\lambda$.\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{W}\\mathbf{v} = \\lambda\\mathbf{v}\n",
    "\\end{equation}\n",
    "\n",
    "If we have one eigenvector for a matrix, we technically have an infinite amount: every vector along the span of that eigenvector is also an eigenvector. So, we often use the unit vector in that direction to summarize all the eigenvectors along that line.\n",
    "\n",
    "We can find the eigenvalues and eigenvectors of a matrix in numpy using `np.linalg.eig`.\n",
    "\n",
    "### Identifying transformations from eigenvectors\n",
    "\n",
    "Earlier, we learned how to think about linear transformations in terms of where the standard basis vectors end up. We can also think about them in terms of eigenvectors.\n",
    "\n",
    "Just by looking at eigenvectors before and after a transformation, **can you describe what the transformation is in words (e.g.contraction, expansion, horizontal vs vertical, projection onto an axis, reflection, and rotation)**? Try for each of the two plots below.\n",
    "\n",
    "Note that I show an eigenvector for every eigenvalue. The $x/y$ limits do not change in before vs after (so eigenvectors are showed scaled by the eigenvalues)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219ed3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example #1\n",
    "W = np.array([[3, 0], [0, 1]])\n",
    "plot_eig_vec_transform(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68e2dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example #2\n",
    "W = np.array([[0, 1], [1, 0]])\n",
    "plot_eig_vec_transform(W)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbba205",
   "metadata": {},
   "source": [
    "As we saw above, looking at how just the eigenvectors change after a transformation can be very informative about what that transformation was."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed6458d",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## About this notebook\n",
    "Most of the content here is directly adapted from [Neuromatch Academy Materials](https://compneuro.neuromatch.io/tutorials/W0D3_LinearAlgebra/student/W0D3_Tutorial1.html), shared under a Creative Commons Attribution 4.0 International License."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
