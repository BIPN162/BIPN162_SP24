{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb839485",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "In this notebook, we'll explore **logistic regression** as a way of predicting a binary outcome and classifying data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5a6ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our necessary toolboxes\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import scipy.io\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55718865",
   "metadata": {},
   "source": [
    "## Part I: Example with GRE scores\n",
    "\n",
    "<div class=\"alert alert-success\"><b>Task</b>: Below, load in <code>logisticGRE.csv</code> (in our Data folder) using Pandas and assign it to <code>df</code>. Then, add another column <code>gre_adj</code> that is the original GRE scoress minus their mean. Finally, create a scatter plot of the the original scores versus the admitted.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d84690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "# Add mean-adjusted column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e84155",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot scatter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13dcd0a8",
   "metadata": {},
   "source": [
    "Clearly, this data is not appropriate for a linear regression. Let's see if a logistic regression can help us predict which scores lead to admission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12ff8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression #1\n",
    "\n",
    "x = np.array(df['gre_adj'])\n",
    "y = np.array(df['admitted'])\n",
    "\n",
    "logRegression = LogisticRegression() # Set up model\n",
    "logRegression.fit(x.reshape(len(x),1),y.reshape(len(y),1)) # Fit model\n",
    "\n",
    "x_pred = np.arange(x.min(),x.max(),1) # Create a range of values to predict\n",
    "\n",
    "y_pred = logRegression.predict_proba(x_pred.reshape(len(x_pred),1)) # Predict y values\n",
    "\n",
    "y_proba = np.array([prob[1] for prob in y_pred]) # Just take the second value (for 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43dfb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x_pred+np.mean(x_orig),y_pred[:,1],c='k') #1\n",
    "plt.title('Predicted Grad school admittance as function of GRE scores') #2\n",
    "plt.xlabel('GRE scores') #3\n",
    "plt.ylabel('Admitted') #4\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b56cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(gre_adj.reshape(-1, 1), admitted, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train the logistic regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict probabilities for the test set\n",
    "y_prob = model.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4228e869",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "auc_score = roc_auc_score(y_test, y_prob)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {auc_score:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc690f5b",
   "metadata": {},
   "source": [
    "## Part 2: Back to our simulated neural population\n",
    "\n",
    "In the last notebook, we used PCA followed by K-Means clustering to see if we could identify different cell types within our sample. Below, we'll give it a try with logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ab5a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify fields to generate data\n",
    "generatorFields=['type_num', 'transmission', 'latMean','latStd','volMean','volStd', 'depthMean', \\\n",
    "                 'depthStd','maxrateMean', 'maxrateStd','spontMean','spontStd','widthMean','widthStd', \\\n",
    "                 'axonMean','axonStd','dendriteMean','dendriteStd']\n",
    "\n",
    "# Specify each property value\n",
    "Type1=[1, 'excitatory', 14, .5, 150,30,500,20,.9, .1,.02,.01, 1, .05, 160, 20, 180,30 ] #Cortical Layer 4 pyramid\n",
    "Type2=[2, 'excitatory', 15, .5, 120,30,300,20,.8, .1,.07,.01, 1, .04, 150, 20, 150,30 ] #L2 pyramidal\n",
    "Type3=[3, 'inhibitory', 15, 1, 120,30, 300,20, .95,.1,.2,.1, .2,.001,150, 10, 150,10 ] #L2 inhibitory PV+\n",
    "Type4=[4, 'inhibitory', 17, 4, 110,30,300,20, .3, .1,.02, .01,.3,.005,150, 10, 150,40 ] #L2 inhibitory SOM+ \n",
    "Type5=[5, 'excitatory', 22, 5, 180,20,800,100,.35,.2,.35, .1,.5,.1, 1000, 500, 200,60 ] #L6 excitatory pyramid\n",
    "Type6=[6, 'inhibitory', 13, .5, 100,30,500,20,.95,.1,.2,.1, .2,.001,150, 10, 150,10 ] #L4 inhibitory FS+\n",
    "\n",
    "dftype = pd.DataFrame([Type1,Type2,Type3,Type4,Type5,Type6],columns=generatorFields)\n",
    "dftype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0364610b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cellFields = ['latency','volume','depth','maxrate','spont','width','axon','dendrite','transmission']\n",
    "dataset = pd.DataFrame(columns = cellFields) # Inititialize our dataset\n",
    "\n",
    "for i in range(100): # For one hundred neurons\n",
    "    tt = np.random.randint(6) # Randomly choose a cell type\n",
    "    trans = dftype.loc[tt,'transmission'] # Excitatory or inhibitory?\n",
    "    latency = dftype.loc[tt,'latMean']+np.random.randn()*dftype.loc[tt]['latStd'] # Mean latency, with some jitter\n",
    "    vol=dftype.loc[tt,'volMean']+np.random.randn()*dftype.loc[tt]['volStd'] # Mean volume, with some jitter\n",
    "    z = dftype.loc[tt,'depthMean']+np.random.randn()*dftype.loc[tt]['depthStd'] # Mean depth, with some jitter\n",
    "    maxrate = dftype.loc[tt,'maxrateMean']+np.random.randn()*dftype.loc[tt]['maxrateStd']\n",
    "    spont = dftype.loc[tt,'spontMean']+np.random.randn()*dftype.loc[tt]['spontStd'] \n",
    "    waveWidth = dftype.loc[tt,'widthMean']+np.random.randn()*dftype.loc[tt]['widthStd']\n",
    "    axon = dftype.loc[tt,'axonMean']+np.random.randn()*dftype.loc[tt]['axonStd'] \n",
    "    dendrite = dftype.loc[tt,'dendriteMean']+np.random.randn()*dftype.loc[tt]['dendriteStd']\n",
    "    \n",
    "    # Append each simulated property to the dataset\n",
    "    dataset = pd.concat([dataset,pd.DataFrame\n",
    "    ([[latency,vol,z,maxrate,spont,waveWidth,axon,dendrite,trans]],columns=cellFields)],ignore_index=True)\n",
    "    \n",
    "print(dataset.shape)\n",
    "nrows,ncolumns = dataset.shape\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d84f399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Data\n",
    "X = dataset.iloc[:, :-1].values \n",
    "y = dataset.iloc[:, -1].values \n",
    "\n",
    "# Create binary labels for logistic regression\n",
    "y_binary = np.where(y == 'excitatory', 0, 1) \n",
    "\n",
    "# Divide into train and test sets and standardize values of each\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_binary, test_size=0.20, random_state=0)\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=2) \n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "# Create DataFrame for plotting with seaborn\n",
    "df_train_pca = pd.DataFrame(data=X_train_pca, columns=['PC1', 'PC2'])\n",
    "df_train_pca['Transmission'] = y_train.astype(str)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3883c1ea",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\"><b>Task</b>: Fit a logistic regression model to the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9349838d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Fit Logistic Regression\n",
    "model = LogisticRegression()\n",
    "model.fit(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9f0ff9",
   "metadata": {},
   "source": [
    "Now, we can visualize our results by using coloring each point by the probability of being excitatory or inhibitory, and using the coefficients to reconstruct the logistic regression line in PCA space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0826a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict Probabilities\n",
    "df_train_pca['Probability'] = model.predict_proba(X_train_pca)[:, 1] \n",
    "\n",
    "# Visualize with lmplot\n",
    "plot = sns.lmplot(x='PC1', y='PC2', hue='Probability', data=df_train_pca, fit_reg=False, palette='coolwarm', legend=False)\n",
    "ax = plot.axes[0, 0]\n",
    "\n",
    "# Normalize probabilities to 0-1 range for colormap\n",
    "norm = plt.Normalize(df_train_pca['Probability'].min(), df_train_pca['Probability'].max())\n",
    "\n",
    "# Get scatter plot points and apply colormap\n",
    "points = ax.collections[0]\n",
    "points.set_cmap('coolwarm')\n",
    "points.set_norm(norm)  \n",
    "\n",
    "# Create colorbar\n",
    "cbar = plt.colorbar(points)\n",
    "cbar.ax.set_ylabel('Probability of Inhibitory', rotation=270, labelpad=15)\n",
    "\n",
    "# Overlay decision boundary\n",
    "b = model.intercept_[0]\n",
    "w1, w2 = model.coef_.T\n",
    "c = -b/w2\n",
    "m = -w1/w2\n",
    "xd = np.array([df_train_pca['PC1'].min(), df_train_pca['PC1'].max()])\n",
    "yd = m*xd + c\n",
    "plt.plot(xd, yd, 'k', lw=1, ls='--') \n",
    "\n",
    "plt.title('Logistic Regression Decision Boundary')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be500db8",
   "metadata": {},
   "source": [
    "We can compare our logistic regression findings with the ground truth..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f5dbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(x='PC1', y='PC2', hue='Transmission', data=df_train_pca, fit_reg=False, palette='coolwarm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3ac58e",
   "metadata": {},
   "source": [
    "### About this notebook\n",
    "The examples in this notebook are based off of those in *Neural Data Science* Chapters 7 and 9."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
